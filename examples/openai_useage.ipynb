{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(\"src\"), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:24:02,736 - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n",
      "/Users/amirdor/Documents/LLMOpt/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nadir.llm_selector.selector.auto import AutoSelector\n",
    "from nadir.config.settings import ModelConfig, DynamicLLMSelectorConfig\n",
    "from nadir.llm_selector.model_registry import ModelRegistry, ModelConfig\n",
    "from nadir.compression.openai import OpenaiCompressor\n",
    "from nadir.compression.gemini import GeminiCompressor\n",
    "from nadir.llm_selector.providers.openai import OpenAIProvider\n",
    "from nadir.llm_selector.providers.gemini import GeminiProvider\n",
    "from nadir.llm_selector.providers.anthropic import AnthropicProvider\n",
    "from nadir.complexity.llm import LLMComplexityAnalyzer\n",
    "from nadir.complexity.code import CodeComplexityAnalyzer\n",
    "from nadir.compression.code import CodeCompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "compressor = GeminiCompressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:24:10,562 - INFO - Loaded 284 candidates from /Users/amirdor/Documents/LLMOpt/src/complexity/../config/model_performance.json\n",
      "2025-02-07 10:24:10,563 - INFO - Initialized LLMComplexityAnalyzer with 3 candidate(s).\n",
      "2025-02-07 10:24:10,563 - INFO - Initialized GeminiComplexityAnalyzer with model: gemini/gemini-1.5-flash-8b\n"
     ]
    }
   ],
   "source": [
    "candidate_names = [\"openai/gpt-4o\", \"openai/gpt-4o-mini\", \"gemini/gemini-1.5-flash\"]\n",
    "analyzer = LLMComplexityAnalyzer(candidate_names=candidate_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:24:12,738 - INFO - Loaded model performance config from /Users/amirdor/Documents/LLMOpt/src/llm_selector/selector/../../config/model_performance.json\n",
      "2025-02-07 10:24:12,739 - INFO - AutoSelector initialized with 24 models.\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM Selector with custom configuration\n",
    "llm_selector = AutoSelector(complexity_analyzer=analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openai/gpt-4o-mini',\n",
       " 'openai/gpt-4',\n",
       " 'anthropic/claude-2.0',\n",
       " 'openai/gpt-4-turbo',\n",
       " 'anthropic/claude-3-sonnet-20240229',\n",
       " 'anthropic/claude-3-5-sonnet-20241022',\n",
       " 'gemini/gemini-1.5-pro-002',\n",
       " 'openai/o1-mini-2024-09-12',\n",
       " 'gemini/gemini-1.5-pro-001',\n",
       " 'gemini/gemini-pro',\n",
       " 'anthropic/claude-3-5-sonnet-20240620',\n",
       " 'gemini/gemini-1.5-flash-001',\n",
       " 'anthropic/claude-3-5-haiku-20241022',\n",
       " 'openai/o3-mini',\n",
       " 'openai/gpt-4o-2024-11-20',\n",
       " 'openai/gpt-4o-2024-08-06',\n",
       " 'anthropic/claude-3-opus-20240229',\n",
       " 'gemini/gemini-1.5-flash',\n",
       " 'anthropic/claude-2.1',\n",
       " 'openai/o1-preview-2024-09-12',\n",
       " 'gemini/gemini-2.0-flash-exp',\n",
       " 'gemini/gemini-exp-1114',\n",
       " 'anthropic/claude-3-haiku-20240307',\n",
       " 'openai/gpt-4o']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_selector.model_registry.get_models_full_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "prompts = [\n",
    "    # Lovable.dev prompts\n",
    "    \"\"\"An app for salads business online.\n",
    "Name: Leaf & Grain\n",
    "Background: leafy green and millets and grains(green and beige)\n",
    "\n",
    "2 Options\n",
    "Option 1 : Made For you\n",
    "Option 2: Build your Bowl\n",
    "made of you page has 3 selection .Bowl size Small,Medium and Large with prefixed Menus of salads .\n",
    "Build Your Bowl \n",
    "Step 1:Customer opts for a size of the bowl.(Small,Med & large)(volume of bowls per Ingredient is Prefixed)\n",
    "Step 2:Select a Base of Available(ice burg Lettuce,Green Lettuce,Cabbage,brown Rice,Quinoa,Boiled millets and Sprouts)\n",
    "Step 3:Choice of other Veggies(Peppers,Red cabbage,Tomatoes,Cherry tomoatoes,Steamed Broccolli,Sweat Potato,celery,carrots,Beetroot,Apples,mushrooms,pineapples,etc.\n",
    "Step 4:top it up with choice of Sauces and mayo(All freshly made)or freshly made as per order.\n",
    "Step 5:Choice of Protien from Veg & non veg Options( Chicken,Lamb,Eggs,Soy,Boiled Chickpeas,Cottage cheese.\n",
    "Step 6:Top it off with a drizzle of Extra Virgin Olive Oil,Sesame Oil,virgin coconut oil and a dash of Sesame Seeds,Chai seeds,Tulasi Seeds,All seed mix(Flax seeds,Pumpkin,Sunflower).\n",
    "       Basil,corinander,mint,parsley.\n",
    "Salt,Pepper, Chilli Flakes,Vinegar(plain and Apple Cider),Oregano & Green chilli oil available for customers as Condiments.\n",
    "Customer selects the Bowl ,then customers has selection for base like rice ,Millets etc to select from.Then the Veggies base.\"\"\",\n",
    "    \"\"\"make these changes in the Made of you Tab.Option 1 : Made For you(customer has 3 option of Bowl select from. small,medium and large.once the selection is done the menu transits into the Names of salads Mentioned Below.\n",
    "Chicken satay salad,Crunchy chopped salad,Tuna, asparagus & white bean salad,Vegan roast spiced squash salad with tahini dressing,Giant couscous salad with charred veg & tangy pesto,Peanut lime salad,Lentil & tuna salad,Epic summer salad,Celery salad,Chickpea salad paste these with Pictures\"\"\"\n",
    "    \"\"\"Home Page Text\n",
    "Header Section\n",
    "Tagline:\n",
    "\"Build Connections That Matter â€“ Where Prestige Meets Opportunity.\"\n",
    "\n",
    "Subtext:\n",
    "Join a community of professionals driven by trust, integrity, and high achievement. Discover, connect, and grow like never before.\n",
    "\n",
    "Call-to-Action Buttons:\n",
    "\n",
    "[Get Started]\n",
    "[Explore Membership Options]\n",
    "Benefits Section\n",
    "Headline:\n",
    "\"Why Join Us?\"\n",
    "\n",
    "Bullet Points:\n",
    "\n",
    "ðŸŒŸ Exclusive Network: Connect with top professionals in your industry and location.\n",
    "ðŸ’¬ Meaningful Discussions: Join subject-based chats and forums tailored to your interests.\n",
    "ðŸ… Prestige & Rewards: Earn recognition and points for engagement, referrals, and contributions.\n",
    "ðŸ›¡ï¸ Privacy First: Complete control over your profile visibility and interactions.\n",
    "Membership Tiers Section\n",
    "Headline:\n",
    "\"Choose Your Membership Level\"\n",
    "\n",
    "Tiers:\n",
    "\n",
    "Basic â€“ Â£2/day\n",
    "\n",
    "Access to the community.\n",
    "Connect with users in your location.\n",
    "Pro â€“ Â£7/day\n",
    "\n",
    "All Basic features.\n",
    "Access exclusive professional events and industry discussions.\n",
    "Message and connect with higher-tier users.\n",
    "Elite â€“ Â£14/day\n",
    "\n",
    "All Pro features.\n",
    "Invitation-only connections and VIP events.\n",
    "Full visibility and networking with top-tier professionals.\n",
    "CTA Buttons for Each Tier:\n",
    "\n",
    "[Join Basic]\n",
    "[Upgrade to Pro]\n",
    "[Go Elite]\n",
    "Dynamic Section: Testimonials\n",
    "Headline:\n",
    "\"What Our Members Say\"\n",
    "\n",
    "\"This platform connected me with mentors I couldnâ€™t find elsewhere.\" â€“ Sarah, Tech Entrepreneur\n",
    "\"Finally, a network where quality outweighs quantity.\" â€“ James, Investment Banker\n",
    "Footer Section\n",
    "Links:\n",
    "About Us | Contact | Terms & Conditions | Privacy Policy\n",
    "Social Media: Icons for LinkedIn, Twitter, Instagram\n",
    "Tagline:\n",
    "\"Great leaders have great followers.\"\n",
    "Dynamic Website Instructions\n",
    "1. Membership Tier Selection\n",
    "Each membership tier button ([Join Basic], [Upgrade to Pro], [Go Elite]) should redirect to the registration page, passing the selected tier as a parameter.\n",
    "Example URL: /register?tier=basic\n",
    "On the registration page:\n",
    "Display the selected membership tier.\n",
    "Allow users to change tiers before completing registration.\n",
    "Implementation:\n",
    "\n",
    "Frontend:\n",
    "Use React Router for navigation.\n",
    "Pass the selected tier as a query parameter.\n",
    "Backend:\n",
    "Store the tier in the user's profile once registration is complete.\n",
    "Dynamic Text on Registration Page:\n",
    "\"Youâ€™ve selected the {tier} membership. Complete your registration to start networking!\"\n",
    "2. Personalized Home Page Content\n",
    "Logged-Out View:\n",
    "Show a static mock-up of the platform with general benefits and tier information.\n",
    "Logged-In View:\n",
    "Replace the homepage with a dashboard showing:\n",
    "Recommended connections (based on geolocation and industry).\n",
    "Discussion threads and events based on the user's interests.\n",
    "Their current membership tier and option to upgrade.\n",
    "Implementation:\n",
    "\n",
    "Use Supabaseâ€™s authentication status to check if the user is logged in.\n",
    "Redirect logged-in users to the dashboard.\n",
    "3. Dynamic Testimonials Section\n",
    "Rotate user testimonials dynamically.\n",
    "Use Supabase to store and fetch testimonials.\n",
    "Implementation:\n",
    "\n",
    "Supabase table:\n",
    "testimonials: id, user_name, testimonial_text, user_title\n",
    "Display a random testimonial using Supabaseâ€™s API:\n",
    "javascript\n",
    "Copy\n",
    "Edit\n",
    "const fetchTestimonial = async () => {\n",
    "  const { data, error } = await supabase.from('testimonials').select('*').limit(1).single();\n",
    "  if (data) setTestimonial(data);\n",
    "};\n",
    "4. Payment and Registration Integration\n",
    "Stripe Integration:\n",
    "Add a step on the registration page to process payments based on the selected tier.\n",
    "Once payment is successful, complete the user registration.\n",
    "Flow:\n",
    "\n",
    "User selects a tier and is redirected to /register.\n",
    "After filling out profile details, the user clicks Submit.\n",
    "Redirect to Stripe for payment.\n",
    "On payment success, save the userâ€™s data in the Supabase database and send a welcome email.\n",
    "Implementation:\n",
    "\n",
    "Frontend: Use Stripeâ€™s Checkout API for payment processing.\n",
    "Backend:\n",
    "Handle webhooks from Stripe to verify payments.\n",
    "Update the userâ€™s membership tier in the database.\"\"\",\n",
    "\"\"\"\n",
    "History\n",
    "### **Project Name:** Retro Basketball Scoreboard App  \n",
    "\n",
    "---\n",
    "\n",
    "### **Project Description:**  \n",
    "Develop a **Retro Basketball Scoreboard App** that immerses users in the classic vibe of old-school basketball scoreboards while providing robust functionality for real-time game tracking. The app will allow users to manage scores, fouls, timers, and player information, including substitutions, personal fouls, and individual performance metrics. A registration system for teams and players will enhance pre-game preparation.  \n",
    "\n",
    "The app will be highly interactive, featuring a nostalgic retro design with analog-inspired elements and engaging animations. All data will be managed locally in the browser without external database dependencies.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Features to Include:**  \n",
    "\n",
    "#### **1. Team and Player Registration**  \n",
    "- Users can **add teams** and input their **team names** dynamically.  \n",
    "- **Player registration for each team**, including:  \n",
    "  - **Player Name**  \n",
    "  - **Jersey Number**  \n",
    "  - Initial foul and score count set to 0.  \n",
    "- Display team rosters with all registered players before starting the game.  \n",
    "\n",
    "#### **2. Editable Team and Player Information**  \n",
    "- Allow users to edit:  \n",
    "  - **Team names**  \n",
    "  - **Player information** (e.g., name, jersey number).  \n",
    "- Real-time updates to reflect changes in the scoreboard interface.  \n",
    "\n",
    "#### **3. Scoring System**  \n",
    "- Track individual and team scores:  \n",
    "  - **+1 Point Button** for individual players.  \n",
    "  - Team score automatically updates based on individual scores.  \n",
    "- Animated score changes with retro flip-card effects.  \n",
    "\n",
    "#### **4. Foul Tracking**  \n",
    "- Track fouls at both **team** and **individual player** levels:  \n",
    "  - **Add Foul Button** for each player, updating personal and team fouls.  \n",
    "  - Notify users when a player accumulates **5 personal fouls**, suggesting substitution.  \n",
    "  - Highlight team fouls when they reach **5 total fouls**, with a retro buzzer effect.  \n",
    "- Visual indicators (e.g., flashing red for players at foul limits).  \n",
    "\n",
    "#### **5. Player Substitution**  \n",
    "- Allow users to substitute players:  \n",
    "  - Replace a fouled-out player with another registered player.  \n",
    "  - Substitute players can inherit the position while their stats reset or continue.  \n",
    "\n",
    "#### **6. Game Timer**  \n",
    "- Display a **retro game timer** with Start, Stop, and Reset controls:  \n",
    "  - Count in **mm:ss** format with an optional quarter indicator (e.g., Q1, Q2).  \n",
    "  - Styled in retro LED digits or analog clock visuals.  \n",
    "\n",
    "#### **7. Shot Clock**  \n",
    "- Include a **24-second shot clock**:  \n",
    "  - Resettable via a button with automatic reset after each scoring play.  \n",
    "  - Buzzer and visual indicator when the shot clock expires.  \n",
    "\n",
    "#### **8. Dynamic Team and Game Stats Display**  \n",
    "- Show team details with retro aesthetics:  \n",
    "  - Team names, total scores, and total fouls displayed prominently.  \n",
    "  - List of players for each team with:  \n",
    "    - Individual scores.  \n",
    "    - Jersey numbers.  \n",
    "    - Personal fouls.  \n",
    "\n",
    "#### **9. Responsive and Interactive UI**  \n",
    "- Large, easy-to-use buttons for actions like scoring, fouling, and substitutions.  \n",
    "- Retro design elements, including:  \n",
    "  - Flip animations for score updates.  \n",
    "  - LED-styled displays for timers and stats.  \n",
    "  - Tactile button press effects and vintage buzzer sounds.  \n",
    "- Dark mode toggle for evening games with glowing effects on UI elements.  \n",
    "\n",
    "#### **10. Offline Data Management**  \n",
    "- All game data stored locally in the browser using Web Storage (e.g., `localStorage`):  \n",
    "  - Persist data for team rosters, scores, and fouls until manually reset.  \n",
    "  - Allow users to reload or resume the app mid-game without data loss.  \n",
    "\n",
    "---\n",
    "\n",
    "### **UI/UX Design:**  \n",
    "\n",
    "#### **Retro Aesthetic:**  \n",
    "- **Color Scheme:** Warm tones (orange, brown, black) with contrasting white or cream accents.  \n",
    "- **Fonts:** Mimic classic scoreboard fonts with bold, clean readability.  \n",
    "- **Animations:** Smooth transitions for scoring, fouls, and timer updates, styled after vintage mechanical boards.  \n",
    "\n",
    "#### **Game Layout:**  \n",
    "- **Central Display:**  \n",
    "  - Team scores, total fouls, and game timer at the top.  \n",
    "  - Player stats displayed below, with individual fouls and scores clearly visible.  \n",
    "- **Shot Clock Placement:** Prominently displayed near the timer for quick visibility.  \n",
    "- **Buttons:** Large, tactile buttons for scoring and fouling, styled like physical controls.  \n",
    "\n",
    "#### **Responsive Design:**  \n",
    "- Scales across devices:  \n",
    "  - Mobile: Compact layout with swipe or tap functionality.  \n",
    "  - Desktop: Full-featured view with extra decorative elements.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Expected Output:**  \n",
    "A fully functional **Retro Basketball Scoreboard App** that combines the nostalgia of classic scoreboard designs with real-time game management. The app will be intuitive, visually engaging, and provide all essential tools for tracking and managing basketball games effectively. It will be a standalone solution, perfect for amateur leagues, school games, or casual basketball events.  \"\"\",\n",
    "\n",
    "    # Tabnine use cases\n",
    "    \"def quicksort(arr):\\n    \\\"\\\"\\\"Perform a quicksort on a list.\\n\\n    Args:\\n        arr (list): List to sort.\\n\\n    Returns:\\n        list: Sorted list.\\n    \\\"\\\"\\\"\\n    if len(arr) <= 1:\\n        return arr\\n    pivot = arr[len(arr) // 2]\\n    left = [x for x in arr if x < pivot]\\n    middle = [x for x in arr if x == pivot]\\n    right = [x for x in arr if x > pivot]\\n    return quicksort(left) + middle + quicksort(right)\",\n",
    "\n",
    "    \"class Node:\\n    def __init__(self, data):\\n        self.data = data\\n        self.next = None\\n\\nclass LinkedList:\\n    def __init__(self):\\n        self.head = None\\n\\n    def insert(self, data):\\n        new_node = Node(data)\\n        new_node.next = self.head\\n        self.head = new_node\\n\\n    def print_list(self):\\n        current = self.head\\n        while current:\\n            print(current.data)\\n            current = current.next\",\n",
    "\n",
    "    \"import React from 'react';\\n\\nfunction App() {\\n  const [count, setCount] = React.useState(0);\\n\\n  return (\\n    <div>\\n      <p>You clicked {count} times</p>\\n      <button onClick={() => setCount(count + 1)}>Click me</button>\\n    </div>\\n  );\\n}\\n\\nexport default App;\",\n",
    "\n",
    "    \"CREATE TABLE Users (\\n    ID INT PRIMARY KEY AUTO_INCREMENT,\\n    Username VARCHAR(50) NOT NULL,\\n    PasswordHash CHAR(64) NOT NULL,\\n    CreatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP\\n);\",\n",
    "\n",
    "    \"from sklearn.linear_model import LinearRegression\\nimport numpy as np\\n\\n# Create data\\nX = np.array([[1], [2], [3], [4]])\\ny = np.array([2.5, 3.6, 4.5, 5.1])\\n\\n# Fit model\\nmodel = LinearRegression()\\nmodel.fit(X, y)\\n\\n# Predict\\nprint(model.predict([[5]]))\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:18:58,066 - INFO - Sending model selection prompt to gemini/gemini-1.5-flash-8b\n",
      "\u001b[92m10:18:58 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:18:58,071 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:18:59,821 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:18:59 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:18:59,826 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:18:59,828 - INFO - Received model selection response.\n",
      "2025-02-07 10:18:59,830 - INFO - Selected model: gemini-1.5-flash\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': 'An app for salads business online.\\nName: Leaf & Grain\\nBackground: leafy green and millets and grains(green and beige)\\n\\n2 Options\\nOption 1 : Made For you\\nOption 2: Build your Bowl\\nmade of you page has 3 selection .Bowl size Small,Medium and Large with prefixed Menus of salads .\\nBuild Your Bowl \\nStep 1:Customer opts for a size of the bowl.(Small,Med & large)(volume of bowls per Ingredient is Prefixed)\\nStep 2:Select a Base of Available(ice burg Lettuce,Green Lettuce,Cabbage,brown Rice,Quinoa,Boiled millets and Sprouts)\\nStep 3:Choice of other Veggies(Peppers,Red cabbage,Tomatoes,Cherry tomoatoes,Steamed Broccolli,Sweat Potato,celery,carrots,Beetroot,Apples,mushrooms,pineapples,etc.\\nStep 4:top it up with choice of Sauces and mayo(All freshly made)or freshly made as per order.\\nStep 5:Choice of Protien from Veg & non veg Options( Chicken,Lamb,Eggs,Soy,Boiled Chickpeas,Cottage cheese.\\nStep 6:Top it off with a drizzle of Extra Virgin Olive Oil,Sesame Oil,virgin coconut oil and a dash of Sesame Seeds,Chai seeds,Tulasi Seeds,All seed mix(Flax seeds,Pumpkin,Sunflower).\\n       Basil,corinander,mint,parsley.\\nSalt,Pepper, Chilli Flakes,Vinegar(plain and Apple Cider),Oregano & Green chilli oil available for customers as Condiments.\\nCustomer selects the Bowl ,then customers has selection for base like rice ,Millets etc to select from.Then the Veggies base.',\n",
       " 'complexity_score': 65,\n",
       " 'selected_model': 'gemini-1.5-flash',\n",
       " 'details': {'recommended_model': 'gemini/gemini-1.5-flash',\n",
       "  'overall_complexity': 65,\n",
       "  'explanation': \"gemini-1.5-flash strikes a good balance of performance, speed, and cost.  Its higher throughput (168.8 MedianTokens/s) and faster first chunk (0.4s) make it preferable for this potentially moderately complex prompt generation task, especially given the detailed specification involved in describing the app's features.  Its relatively low blended price per 1M tokens (1.3e-07 USD) further enhances the cost-effectiveness compared to the openai/gpt-4o models. Although the openai/gpt-4o models are stronger in quality benchmarks, gemini-1.5-flash's combination of performance and efficiency makes it the more economical choice without compromising significantly on quality.\",\n",
       "  'alternative_model': ['openai/gpt-4o-mini', 'openai/gpt-4o']}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompts[0]\n",
    "llm_selector.get_complexity_details(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:19:03,221 - INFO - Sending model selection prompt to gemini/gemini-1.5-flash-8b\n",
      "\u001b[92m10:19:03 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:19:03,223 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:19:05,752 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:05 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:19:05,757 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:19:05,759 - INFO - Received model selection response.\n",
      "2025-02-07 10:19:05,761 - INFO - Selected model: gemini-1.5-flash\n",
      "\u001b[92m10:19:05 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:19:05,762 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:19:13,148 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:19:13 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:19:13,154 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:19:13,157 - INFO - Response generated using gemini-1.5-flash\n",
      "2025-02-07 10:19:13,162 - INFO - Added input cost: $0.000028, output cost: $0.000272, total: $0.000300. Running total: input $0.000028, output $0.000272, total $0.000300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Leaf & Grain App Design:\\n\\nHere\\'s a proposed design for the Leaf & Grain app, incorporating your specifications:\\n\\n**I. Onboarding/Home Screen:**\\n\\n* **App Logo:**  A visually appealing logo incorporating leafy greens (dark green) and grains (beige/light brown) in a stylized way.\\n* **Headline:**  \"Leaf & Grain: Your Fresh, Customizable Salad Experience\"\\n* **Hero Image:**  A high-quality image of a vibrant, colorful salad.\\n* **Buttons:** \"Made For You\" and \"Build Your Bowl\" (prominently displayed)\\n* **Optional:**  \"View Our Menu,\" \"Special Offers\" (rotating banners), \"Location/Delivery Info\"\\n\\n\\n**II. Option 1: Made For You**\\n\\n* **Headline:** \"Our Signature Salads\"\\n* **Grid Display:** Three salad options displayed with high-quality images, names, and short descriptions (e.g., \"Mediterranean Delight,\" \"Spicy Quinoa Bowl,\" \"Classic Chicken Caesar\").  Each option clearly indicates the size (Small, Medium, Large) and price.\\n* **Size Selection:**  Radio buttons or a dropdown menu to choose the size of the selected salad (Small, Medium, Large).  Prices will automatically update based on the selection.\\n* **Add to Cart Button:**  Clear and prominent button for each salad.\\n\\n\\n**III. Option 2: Build Your Bowl**\\n\\nThis section will use a step-by-step approach with clear instructions and visuals.\\n\\n* **Step 1: Choose Your Bowl Size**\\n    * Radio buttons or a dropdown menu: Small, Medium, Large (with visual representation of bowl sizes if possible)  Prices shown next to each option.\\n* **Step 2: Select Your Base**\\n    * Grid of images with names: Iceberg Lettuce, Green Lettuce, Cabbage, Brown Rice, Quinoa, Boiled Millets, Sprouts.\\n* **Step 3: Choose Your Veggies**\\n    * Scrollable list or grid of veggies with images and checkboxes (allowing multiple selections).  Consider categorizing vegetables (e.g., Leafy Greens, Cruciferous, Root Vegetables, etc.) for better organization.\\n* **Step 4: Dress It Up!**\\n    * Scrollable list or grid of sauces and mayonnaises with images.  Possibly specify if they\\'re freshly made or made to order.\\n* **Step 5: Protein Power**\\n    * Scrollable list or grid with images and checkboxes: Chicken, Lamb, Eggs, Soy, Boiled Chickpeas, Cottage Cheese.  Clearly indicate vegetarian/non-vegetarian options.\\n* **Step 6: The Finishing Touches**\\n    * Scrollable list or grid with images and checkboxes of oils, seeds, and herbs.  Possibly group them into categories (e.g., Oils, Seeds, Herbs).\\n* **Condiments (Step 7 - Optional):**\\n    * Checkboxes for Salt, Pepper, Chilli Flakes, Vinegar (plain & apple cider), Oregano, Green Chilli Oil.\\n\\n\\n**IV. Cart & Checkout:**\\n\\n* **Cart Summary:** Clear display of all selected items, quantities, and prices.\\n* **Subtotal, Tax, Delivery Fee (if applicable), Total:** Clearly displayed.\\n* **Checkout Process:**  Integration with a secure payment gateway (Stripe, PayPal, etc.).\\n* **Delivery/Pickup Options:**  Option to select delivery or pickup with relevant information (address, time slots, etc.)\\n\\n\\n**V.  Account & Profile:**\\n\\n* **User Login/Registration:**  Secure login and registration options.\\n* **Order History:**  View past orders.\\n* **Saved Addresses:**  Save delivery addresses for faster checkout.\\n* **Payment Methods:**  Manage saved payment methods.\\n\\n\\n**VI.  Technical Considerations:**\\n\\n* **Platform Compatibility:**  Design for both iOS and Android.\\n* **User-Friendly Interface:** Intuitive navigation and clear visual hierarchy.\\n* **High-Quality Images:**  Use appealing and high-resolution images of salads and ingredients.\\n* **Performance Optimization:**  Ensure the app loads quickly and runs smoothly.\\n\\n\\nThis detailed outline will help developers create a user-friendly and efficient app for Leaf & Grain. Remember to conduct thorough user testing to refine the design and ensure a smooth user experience.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_selector.generate_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_selector.model_registry.get_sorted_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    # print(f\"\\nPrompt: {prompt}\")\n",
    "    details = llm_selector.get_complexity_details(prompt)\n",
    "    # print(\"Complexity Details:\", details)\n",
    "    # Generate response\n",
    "    # response = llm_selector.generate_response(prompt)\n",
    "    # print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_selector.cost_tracker.get_cost_breakdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = ModelRegistry()\n",
    "mr.register_models([ModelConfig(\n",
    "            name=\"gpt-4o\",\n",
    "            provider=\"openai\",\n",
    "            complexity_threshold=75.0,\n",
    "            model_instance=OpenAIProvider(\"gpt-4o\")\n",
    "        )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:24:22,887 - INFO - Loaded 284 candidates from /Users/amirdor/Documents/LLMOpt/src/complexity/../config/model_performance.json\n",
      "2025-02-07 10:24:22,887 - INFO - Initialized ComplexityAnalyzer with 1 candidate(s).\n",
      "2025-02-07 10:24:22,888 - INFO - Initialized ComplexityAnalyzer with 1 candidate(s).\n"
     ]
    }
   ],
   "source": [
    "from nadir.complexity.analyzer import ComplexityAnalyzer\n",
    "base_analyzer = ComplexityAnalyzer(candidate_names=[\"openai/gpt-4o\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:24:25,434 - INFO - Loaded model performance config from /Users/amirdor/Documents/LLMOpt/src/llm_selector/selector/../../config/model_performance.json\n",
      "2025-02-07 10:24:25,435 - INFO - AutoSelector initialized with 1 models.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gpt-4o']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_4o_only = AutoSelector(model_registry=mr, complexity_analyzer=base_analyzer)\n",
    "openai_4o_only.list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:24:48,350 - INFO - Selected model: gpt-4o\n",
      "2025-02-07 10:24:48,355 - INFO - Selected model: gpt-4o\n",
      "2025-02-07 10:24:48,361 - INFO - Selected model: gpt-4o\n",
      "2025-02-07 10:24:48,363 - INFO - Selected model: gpt-4o\n",
      "2025-02-07 10:24:48,364 - INFO - Selected model: gpt-4o\n",
      "2025-02-07 10:24:48,365 - INFO - Selected model: gpt-4o\n",
      "2025-02-07 10:24:48,366 - INFO - Selected model: gpt-4o\n",
      "2025-02-07 10:24:48,367 - INFO - Selected model: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    # print(f\"\\nPrompt: {prompt}\")\n",
    "    details = openai_4o_only.get_complexity_details(prompt)\n",
    "    # print(\"Complexity Details:\", details)\n",
    "    \n",
    "    # Generate response\n",
    "    # response = openai_4o_only.generate_response(prompt)\n",
    "    # print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_cost': 0.0, 'output_cost': 0.0, 'total_cost': 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_4o_only.cost_tracker.get_cost_breakdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.evaluator.llm import LLMEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = LLMEvaluator(model=\"gemini/gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:37:33,208 - INFO - Sending model selection prompt to gemini/gemini-1.5-flash-8b\n",
      "\u001b[92m10:37:33 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:37:33,212 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:37:34,632 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:37:34 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:37:34,635 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:37:34,637 - INFO - Received model selection response.\n",
      "2025-02-07 10:37:34,638 - INFO - Selected model: gemini-1.5-flash\n",
      "\u001b[92m10:37:34 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:37:34,639 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LLMOpt/src/llm_selector/selector/auto.py:191\u001b[0m, in \u001b[0;36mAutoSelector.generate_response\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m selected_model\u001b[38;5;241m.\u001b[39mmodel_instance:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model instance found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 191\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mselected_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompressed_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse generated using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    194\u001b[0m usage \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/LLMOpt/src/llm_selector/providers/__init__.py:34\u001b[0m, in \u001b[0;36mBaseProvider.generate_with_metadata\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03mGenerate a response for a given prompt with metadata using liteLLM\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m:return: Dictionary containing generated response and metadata\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[0;32m---> 34\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\u001b[38;5;241m.\u001b[39musage,\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mtotal_cost \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response\u001b[38;5;241m.\u001b[39musage, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_cost\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     44\u001b[0m }\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/litellm/utils.py:1032\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1032\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/litellm/main.py:2328\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2320\u001b[0m     gemini_api_key \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2321\u001b[0m         api_key\n\u001b[1;32m   2322\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m get_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2323\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m get_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPALM_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# older palm api key should also work\u001b[39;00m\n\u001b[1;32m   2324\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mapi_key\n\u001b[1;32m   2325\u001b[0m     )\n\u001b[1;32m   2327\u001b[0m     new_params \u001b[38;5;241m=\u001b[39m deepcopy(optional_params)\n\u001b[0;32m-> 2328\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mvertex_chat_completion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   2335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertex_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertex_ai_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertex_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertex_ai_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertex_credentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertex_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgemini_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2350\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex_ai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2351\u001b[0m     vertex_ai_project \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2352\u001b[0m         optional_params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex_project\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   2353\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex_ai_project\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   2354\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mvertex_project\n\u001b[1;32m   2355\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m get_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVERTEXAI_PROJECT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2356\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:1282\u001b[0m, in \u001b[0;36mVertexLLM.completion\u001b[0;34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     client \u001b[38;5;241m=\u001b[39m client\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py:533\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m     req \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, params\u001b[38;5;241m=\u001b[39mparams, headers\u001b[38;5;241m=\u001b[39mheaders, files\u001b[38;5;241m=\u001b[39mfiles, content\u001b[38;5;241m=\u001b[39mcontent  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    532\u001b[0m     )\n\u001b[0;32m--> 533\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Documents/LLMOpt/.venv/lib/python3.10/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1292\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1291\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1165\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm_selector.generate_response(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:37:44,771 - INFO - Selected model: gpt-4o\n",
      "\u001b[92m10:37:44 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-02-07 10:37:44,775 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:38:04,819 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:38:04 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:04,848 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:04,852 - INFO - Response generated using gpt-4o\n",
      "2025-02-07 10:38:04,853 - INFO - Added input cost: $0.000948, output cost: $0.006600, total: $0.007548. Running total: input $0.006040, output $0.035740, total $0.041780\n",
      "2025-02-07 10:38:04,854 - INFO - Sending model selection prompt to gemini/gemini-1.5-flash-8b\n",
      "\u001b[92m10:38:04 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:38:04,855 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:38:06,529 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:38:06 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:06,533 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:06,533 - INFO - Received model selection response.\n",
      "2025-02-07 10:38:06,534 - INFO - Selected model: gemini-1.5-flash\n",
      "\u001b[92m10:38:06 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:38:06,535 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:38:21,387 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:38:21 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:21,401 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:21,403 - INFO - Response generated using gemini-1.5-flash\n",
      "2025-02-07 10:38:21,405 - INFO - Added input cost: $0.000028, output cost: $0.000214, total: $0.000242. Running total: input $0.000301, output $0.002304, total $0.002605\n",
      "\u001b[92m10:38:21 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:38:21,407 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:38:23,217 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:38:23 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:23,221 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:23,235 - INFO - Selected model: gpt-4o\n",
      "\u001b[92m10:38:23 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-02-07 10:38:23,237 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:38:39,563 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:38:39 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:39,580 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:39,583 - INFO - Response generated using gpt-4o\n",
      "2025-02-07 10:38:39,584 - INFO - Added input cost: $0.002250, output cost: $0.006670, total: $0.008920. Running total: input $0.008290, output $0.042410, total $0.050700\n",
      "2025-02-07 10:38:39,584 - INFO - Sending model selection prompt to gemini/gemini-1.5-flash-8b\n",
      "\u001b[92m10:38:39 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:38:39,589 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:38:41,225 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:38:41 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:41,229 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:41,230 - INFO - Received model selection response.\n",
      "2025-02-07 10:38:41,231 - INFO - Selected model: gpt-4o-mini\n",
      "\u001b[92m10:38:41 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2025-02-07 10:38:41,232 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2025-02-07 10:38:52,776 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:38:52 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:52,802 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:52,805 - INFO - Response generated using gpt-4o-mini\n",
      "2025-02-07 10:38:52,807 - INFO - Added input cost: $0.000135, output cost: $0.000565, total: $0.000700. Running total: input $0.000436, output $0.002869, total $0.003305\n",
      "\u001b[92m10:38:52 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:38:52,811 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:38:55,887 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:38:55 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:55,892 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:38:55,907 - INFO - Selected model: gpt-4o\n",
      "\u001b[92m10:38:55 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-02-07 10:38:55,910 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:39:07,407 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:39:07 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:07,421 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:07,426 - INFO - Response generated using gpt-4o\n",
      "2025-02-07 10:39:07,427 - INFO - Added input cost: $0.002743, output cost: $0.006960, total: $0.009703. Running total: input $0.011033, output $0.049370, total $0.060402\n",
      "2025-02-07 10:39:07,428 - INFO - Sending model selection prompt to gemini/gemini-1.5-flash-8b\n",
      "\u001b[92m10:39:07 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:39:07,429 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:39:09,576 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:39:09 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:09,579 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:09,580 - INFO - Received model selection response.\n",
      "2025-02-07 10:39:09,581 - INFO - Selected model: gemini-1.5-flash\n",
      "\u001b[92m10:39:09 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:39:09,581 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:39:16,320 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:39:16 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:16,339 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:16,350 - INFO - Response generated using gemini-1.5-flash\n",
      "2025-02-07 10:39:16,353 - INFO - Added input cost: $0.000087, output cost: $0.000193, total: $0.000280. Running total: input $0.000523, output $0.003061, total $0.003585\n",
      "\u001b[92m10:39:16 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:39:16,357 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:39:18,233 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:39:18 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:18,238 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:18,244 - INFO - Selected model: gpt-4o\n",
      "\u001b[92m10:39:18 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-02-07 10:39:18,246 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:39:26,000 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:39:26 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:26,019 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:26,024 - INFO - Response generated using gpt-4o\n",
      "2025-02-07 10:39:26,026 - INFO - Added input cost: $0.000310, output cost: $0.003280, total: $0.003590. Running total: input $0.011343, output $0.052650, total $0.063992\n",
      "2025-02-07 10:39:26,028 - INFO - Sending model selection prompt to gemini/gemini-1.5-flash-8b\n",
      "\u001b[92m10:39:26 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:39:26,030 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:39:27,146 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:39:27 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:27,148 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:27,149 - INFO - Received model selection response.\n",
      "2025-02-07 10:39:27,150 - INFO - Selected model: gemini-1.5-flash\n",
      "\u001b[92m10:39:27 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:39:27,150 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:39:32,532 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:39:32 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:32,538 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:32,541 - INFO - Response generated using gemini-1.5-flash\n",
      "2025-02-07 10:39:32,542 - INFO - Added input cost: $0.000010, output cost: $0.000187, total: $0.000197. Running total: input $0.000533, output $0.003248, total $0.003781\n",
      "\u001b[92m10:39:32 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:39:32,544 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:39:34,471 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:39:34 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:34,476 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:34,482 - INFO - Selected model: gpt-4o\n",
      "\u001b[92m10:39:34 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-02-07 10:39:34,483 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:39:40,568 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:39:40 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:40,581 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:40,583 - INFO - Response generated using gpt-4o\n",
      "2025-02-07 10:39:40,585 - INFO - Added input cost: $0.000258, output cost: $0.002970, total: $0.003228. Running total: input $0.011600, output $0.055620, total $0.067220\n",
      "2025-02-07 10:39:40,586 - INFO - Sending model selection prompt to gemini/gemini-1.5-flash-8b\n",
      "\u001b[92m10:39:40 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:39:40,587 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:39:42,066 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:39:42 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:42,088 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:42,089 - INFO - Received model selection response.\n",
      "2025-02-07 10:39:42,090 - INFO - Selected model: gemini-1.5-flash\n",
      "\u001b[92m10:39:42 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:39:42,091 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:39:52,550 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:39:52 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:52,572 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:52,574 - INFO - Response generated using gemini-1.5-flash\n",
      "2025-02-07 10:39:52,576 - INFO - Added input cost: $0.000009, output cost: $0.000388, total: $0.000397. Running total: input $0.000542, output $0.003636, total $0.004178\n",
      "\u001b[92m10:39:52 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:39:52,577 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:39:54,786 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:39:54 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:54,790 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:39:54,796 - INFO - Selected model: gpt-4o\n",
      "\u001b[92m10:39:54 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-02-07 10:39:54,797 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:40:00,253 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:40:00 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:00,260 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:00,261 - INFO - Response generated using gpt-4o\n",
      "2025-02-07 10:40:00,263 - INFO - Added input cost: $0.000203, output cost: $0.002950, total: $0.003153. Running total: input $0.011803, output $0.058570, total $0.070372\n",
      "2025-02-07 10:40:00,263 - INFO - Sending model selection prompt to gemini/gemini-1.5-flash-8b\n",
      "\u001b[92m10:40:00 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:40:00,265 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:40:02,300 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:40:02 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:02,302 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:02,304 - INFO - Received model selection response.\n",
      "2025-02-07 10:40:02,306 - INFO - Selected model: gemini-1.5-flash\n",
      "\u001b[92m10:40:02 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:40:02,307 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:40:07,068 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:40:07 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:07,076 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:07,078 - INFO - Response generated using gemini-1.5-flash\n",
      "2025-02-07 10:40:07,079 - INFO - Added input cost: $0.000006, output cost: $0.000138, total: $0.000144. Running total: input $0.000549, output $0.003774, total $0.004322\n",
      "\u001b[92m10:40:07 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:40:07,080 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:40:08,780 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:40:08 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:08,783 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:08,786 - INFO - Selected model: gpt-4o\n",
      "\u001b[92m10:40:08 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-02-07 10:40:08,787 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:40:21,021 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:40:21 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:21,327 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:21,329 - INFO - Response generated using gpt-4o\n",
      "2025-02-07 10:40:21,331 - INFO - Added input cost: $0.000120, output cost: $0.004400, total: $0.004520. Running total: input $0.011923, output $0.062970, total $0.074892\n",
      "2025-02-07 10:40:21,331 - INFO - Sending model selection prompt to gemini/gemini-1.5-flash-8b\n",
      "\u001b[92m10:40:21 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:40:21,332 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:40:22,399 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:40:22 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:22,407 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:22,410 - INFO - Received model selection response.\n",
      "2025-02-07 10:40:22,410 - INFO - Selected model: gemini-1.5-flash\n",
      "\u001b[92m10:40:22 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:40:22,411 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:40:25,384 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:40:25 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:25,387 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:25,388 - INFO - Response generated using gemini-1.5-flash\n",
      "2025-02-07 10:40:25,390 - INFO - Added input cost: $0.000004, output cost: $0.000097, total: $0.000101. Running total: input $0.000552, output $0.003871, total $0.004423\n",
      "\u001b[92m10:40:25 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:40:25,391 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:40:27,695 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:40:27 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:27,698 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:27,703 - INFO - Selected model: gpt-4o\n",
      "\u001b[92m10:40:27 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n",
      "2025-02-07 10:40:27,705 - INFO - \n",
      "LiteLLM completion() model= gpt-4o; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:40:33,737 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:40:33 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:33,745 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:33,757 - INFO - Response generated using gpt-4o\n",
      "2025-02-07 10:40:33,759 - INFO - Added input cost: $0.000223, output cost: $0.002940, total: $0.003163. Running total: input $0.012145, output $0.065910, total $0.078055\n",
      "2025-02-07 10:40:33,760 - INFO - Sending model selection prompt to gemini/gemini-1.5-flash-8b\n",
      "\u001b[92m10:40:33 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:40:33,765 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash-8b; provider = gemini\n",
      "2025-02-07 10:40:35,124 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-8b:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:40:35 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:35,132 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:35,132 - INFO - Received model selection response.\n",
      "2025-02-07 10:40:35,133 - INFO - Selected model: gemini-1.5-flash\n",
      "\u001b[92m10:40:35 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:40:35,134 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:40:39,063 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:40:39 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:39,069 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:39,071 - INFO - Response generated using gemini-1.5-flash\n",
      "2025-02-07 10:40:39,073 - INFO - Added input cost: $0.000007, output cost: $0.000133, total: $0.000140. Running total: input $0.000559, output $0.004003, total $0.004562\n",
      "\u001b[92m10:40:39 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:40:39,075 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-07 10:40:40,949 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyANhhqv1XFG2icw4_Ntnv45HYFrSaJe-mw \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m10:40:40 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "2025-02-07 10:40:40,954 - INFO - Wrapper: Completed Call, calling success_handler\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(\"###############################\")\n",
    "    response1 = openai_4o_only.generate_response(prompt)\n",
    "    response2 = llm_selector.generate_response(prompt)\n",
    "    results.append(evaluator.evaluate_responses(prompt, response_1=response1, response_2=response2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"- Similarity Score: 85\\n- Relevance Score for Response 1: 95\\n- Relevance Score for Response 2: 90\\n- Differences: Both responses provide comprehensive outlines for the Leaf & Grain salad app, covering similar features and functionalities. However, Response 1 is more structured, using a clearer step-by-step approach to describe the app's user flow, including sections for Home Screen, Made For You, Build Your Bowl, Cart and Checkout, and additional features.  Response 2 adopts a more conceptual approach, focusing on design elements like visual appeal and user interface considerations. Response 1 offers more detailed specifications for the app's functionality, while Response 2 emphasizes design aesthetics and user experience improvements.  Essentially, Response 1 is more functional, while Response 2 is more design-focused.\\n\",\n",
       " '- Similarity Score: 95\\n- Relevance Score for Response 1: 100\\n- Relevance Score for Response 2: 100\\n- Differences: Both responses comprehensively address all aspects of the prompt.  The primary difference lies in the formatting and presentation. Response 2 uses a more visually structured format with headings, subheadings, bullet points, and even placeholder image notations (![Image of ...]), making it easier to scan and understand at a glance. Response 1 uses a more paragraph-based structure, which is less visually appealing but still delivers the same information.  Response 2 also includes more explicit references to specific implementation details (e.g., React Router) within its structured outline.  Essentially, Response 2 is a more polished and user-friendly version of Response 1.\\n',\n",
       " \"- Similarity Score: 30\\n- Relevance Score for Response 1: 95\\n- Relevance Score for Response 2: 85\\n- Differences: Response 1 is a detailed development plan outlining phases, tasks, and considerations for building the app.  It focuses heavily on the technical implementation. Response 2 is more of a high-level review of the project proposal itself, offering suggestions and improvements to the proposal's content rather than a technical development plan.  Response 1 directly addresses the prompt's request for a project plan; Response 2 analyzes the prompt's *content* and offers constructive criticism/advice on the proposal, which is a different approach entirely.  While both are relevant, they serve very different purposes.\\n\",\n",
       " '- Similarity Score: 85\\n- Relevance Score for Response 1: 95\\n- Relevance Score for Response 2: 98\\n- Differences: Both responses correctly identify the functionality of the provided quicksort implementation and offer suggestions for improvement, focusing on pivot selection and in-place sorting.  However, Response 2 is more comprehensive.  It structures its analysis into strengths and potential improvements, provides a concrete example of an improved function with random pivot selection, and also suggests an optimization to handle duplicates more efficiently. Response 1 provides a good explanation but lacks the detailed improvements and concrete code example that Response 2 offers.  Response 2 also goes further in explaining the tradeoffs involvedâ€”for example, acknowledging the complexity of an in-place implementation.\\n',\n",
       " '- Similarity Score: 70\\n- Relevance Score for Response 1: 95\\n- Relevance Score for Response 2: 90\\n- Differences: Both responses correctly explain the provided code for a singly linked list.  Response 1 provides a concise explanation and a simple example of usage. Response 2, while also explaining the code, goes further by suggesting and implementing several improvements and additions, including append, delete, search, length methods, and error handling within the delete method, as well as a __str__ method.  Response 1 is more focused on explaining the existing code, whereas Response 2 aims to improve and expand upon it.  The added methods in Response 2 make it more comprehensive but also longer and more complex than Response 1.\\n',\n",
       " '- Similarity Score: 90\\n- Relevance Score for Response 1: 95\\n- Relevance Score for Response 2: 98\\n- Differences: Both responses provide excellent explanations of the provided React code.  Response 2 is slightly better because it explains each line of code individually, offering a more granular breakdown. Response 1 provides a more concise, higher-level overview, grouping explanations into broader concepts (State Initialization, Rendering, Event Handling).  Response 2 also includes a more explicit explanation of how to run the code in a React environment, making it more complete for a beginner.  The difference is mostly stylistic; both responses achieve the goal of explaining the code effectively.\\n',\n",
       " '- Similarity Score: 85\\n- Relevance Score for Response 1: 95\\n- Relevance Score for Response 2: 90\\n- Differences: Both responses accurately describe the provided SQL code and offer relevant security advice regarding password hashing.  However, Response 1 goes further by: 1) explicitly suggesting a more secure hashing algorithm (bcrypt or Argon2) and providing a modified SQL statement incorporating a `UNIQUE` constraint for usernames; 2)  presenting its analysis in a more structured and visually appealing manner using numbered points and headings; and 3)  clearly separating the description of the table structure from the security recommendations. Response 2 is more concise but lacks the concrete improvements and structured presentation of Response 1.  While both mention the importance of strong hashing algorithms and salting, Response 1 more directly addresses the potential weakness of using SHA-256.\\n',\n",
       " \"- Similarity Score: 85\\n- Relevance Score for Response 1: 95\\n- Relevance Score for Response 2: 90\\n- Differences: Both responses accurately describe the code's functionality.  Response 1 provides a more concise explanation, focusing on the code's three main sections (data creation, model fitting, prediction). Response 2 is more verbose, including a section on importing libraries and adding a manual (approximate) calculation of the prediction.  Response 2 also explicitly mentions the requirement for a 2D array input to `model.predict`, a detail omitted in Response 1.  While Response 2 attempts a manual calculation, this adds complexity and doesn't perfectly match the model's output.  Response 1's more focused approach and direct statement of the expected output makes it slightly better.\\n\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 10:19:30,577 - INFO - Loaded model performance config from /Users/amirdor/Documents/LLMOpt/src/llm_selector/selector/../../config/model_performance.json\n",
      "2025-02-07 10:19:30,582 - INFO - Loaded 284 candidates from /Users/amirdor/Documents/LLMOpt/src/complexity/../config/model_performance.json\n",
      "2025-02-07 10:19:30,583 - INFO - Initialized LLMComplexityAnalyzer with 24 candidate(s).\n",
      "2025-02-07 10:19:30,583 - INFO - Initialized GeminiComplexityAnalyzer with model: gemini/gemini-1.5-flash-8b\n",
      "2025-02-07 10:19:30,584 - INFO - AutoSelector initialized with 24 models.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(\"src\"), '..')))\n",
    "from nadir.llm_selector.selector.auto import AutoSelector\n",
    "auto_selector = AutoSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    # ðŸŸ¢ Simple Prompts â†’ GPT-3.5-Turbo\n",
    "    {\"prompt\": \"Create a login page with email and password input fields using React and Tailwind CSS.\", \n",
    "     \"expected_complexity\": 10, \"expected_model\": \"openai/gpt-3.5-turbo\"},\n",
    "\n",
    "    {\"prompt\": \"Build a simple static webpage that displays a list of upcoming events in HTML and CSS.\", \n",
    "     \"expected_complexity\": 8, \"expected_model\": \"openai/gpt-3.5-turbo\"},\n",
    "\n",
    "    {\"prompt\": \"Write a Python function to fetch real-time weather data from OpenWeather API and return the temperature in Celsius.\", \n",
    "     \"expected_complexity\": 15, \"expected_model\": \"openai/gpt-3.5-turbo\"},\n",
    "\n",
    "    {\"prompt\": \"Develop a Flask API with CRUD operations for managing a to-do list stored in an SQLite database.\", \n",
    "     \"expected_complexity\": 20, \"expected_model\": \"openai/gpt-3.5-turbo\"},\n",
    "\n",
    "    # ðŸŸ  Moderate Prompts â†’ GPT-4o-Mini\n",
    "    {\"prompt\": \"Build a serverless backend using AWS Lambda and API Gateway that processes user-uploaded CSV files and stores them in an S3 bucket.\", \n",
    "     \"expected_complexity\": 35, \"expected_model\": \"openai/gpt-4o-mini\"},\n",
    "\n",
    "    {\"prompt\": \"Create an interactive stock market dashboard in Next.js that fetches and displays real-time price movements from an API.\", \n",
    "     \"expected_complexity\": 40, \"expected_model\": \"openai/gpt-4o-mini\"},\n",
    "\n",
    "    {\"prompt\": \"Develop a Scikit-learn pipeline that trains a decision tree model on customer churn data and provides predictions via a REST API.\", \n",
    "     \"expected_complexity\": 50, \"expected_model\": \"openai/gpt-4o-mini\"},\n",
    "\n",
    "    {\"prompt\": \"Implement a real-time chat application using Firebase for authentication and Firestore for message storage, allowing users to send text and images.\", \n",
    "     \"expected_complexity\": 55, \"expected_model\": \"openai/gpt-4o-mini\"},\n",
    "\n",
    "    # ðŸ”´ Complex Prompts â†’ GPT-4o\n",
    "    {\"prompt\": \"Build an AI-driven fraud detection model for a banking system that analyzes transaction patterns and flags suspicious activity using federated learning.\", \n",
    "     \"expected_complexity\": 80, \"expected_model\": \"openai/gpt-4o\"},\n",
    "\n",
    "    {\"prompt\": \"Design a cloud-native microservices architecture for an e-commerce platform, ensuring scalability, event-driven communication (Kafka), and zero-downtime deployment strategies.\", \n",
    "     \"expected_complexity\": 85, \"expected_model\": \"openai/gpt-4o\"},\n",
    "\n",
    "    {\"prompt\": \"Develop a decentralized blockchain-based voting system that ensures security, transparency, and anonymity while preventing double voting and sybil attacks.\", \n",
    "     \"expected_complexity\": 90, \"expected_model\": \"openai/gpt-4o\"},\n",
    "\n",
    "    {\"prompt\": \"Build an AI-assisted code generation tool like GitHub Copilot that provides real-time suggestions based on user input, leveraging OpenAI Codex or Llama 3 models.\", \n",
    "     \"expected_complexity\": 95, \"expected_model\": \"openai/gpt-4o\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai_4o_only' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m----> 2\u001b[0m     response1 \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_4o_only\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate_response(prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m     response2 \u001b[38;5;241m=\u001b[39m auto_selector\u001b[38;5;241m.\u001b[39mgenerate_response(prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(evaluator\u001b[38;5;241m.\u001b[39mevaluate_responses(prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], response_1\u001b[38;5;241m=\u001b[39mresponse1, response_2\u001b[38;5;241m=\u001b[39mresponse2))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'openai_4o_only' is not defined"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    response1 = openai_4o_only.generate_response(prompt[\"prompt\"])\n",
    "    response2 = auto_selector.generate_response(prompt[\"prompt\"])\n",
    "    print(evaluator.evaluate_responses(prompt[\"prompt\"], response_1=response1, response_2=response2))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_cost': 0.012145, 'output_cost': 0.06591, 'total_cost': 0.078055}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_4o_only.cost_tracker.get_cost_breakdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_cost': 0.000559, 'output_cost': 0.004003, 'total_cost': 0.004562}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_selector.cost_tracker.get_cost_breakdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
